Because of the large computational demands of processing transcriptomic data, a HPC system is often used. Remote HPC servers can be accesssed remotely off of a base platform. For my project I used Ubuntu, so this section will include only Ubuntu syntax. See section 3.1 for basic Ubuntu commands. 

Table of Contents:
#Index Genome
#Downloading Transcriptomic Data sets
#Downloading sickle
#Processing reads-single and paired 
#Moving Output .tsv 











#Index Genome
This first step is to find and download the reference genome. This should be done locally. When downloading the genome, the file should be in FASTA format, having a .fasta or .fa extention. When downloading off of NCBI, the file is often stored in a .fna.gz format. To convert the file into the appropriate format, the file must first be unzipped. In Ubuntu, locate the location of the downloaded file and move it to the folder you are working in: I simply used my base directory. 


My code, which moves the downloaded genome from NCBI into my workspace:
----------------------------------------------------------------------------------------
mv /mnt/c/Users/rglad/Downloads/GCF_000006765.1_ASM676v1_genomic.fna.gz /home/rgladwell2
-----------------------------------------------------------------------------------------




Now, when ls is called in the home directory, the file name should appear. We can now unzip the file uzing the command gunzip filename.fna.gz. When ls is called now, the file name should appear without the .gz format. Next the file should be converted from .fna to .fa. Use the command cp filename.fna filename.fa to accomplish this. 

My example code:

------------------------------------------------------------------------------------
 gunzip GCF_000006765.1_ASM676v1_genomic.fna.gz
 cp GCF_000006765.1_ASM676v1_genomic.fna GCF_000006765.1_ASM676v1_genomic.fa
 -----------------------------------------------------------------------------------
 
 
 
 Use the rm command to get rid of any files you no longer need, such as the remaining .fna file.
 
 It is now time to upload the .fa file to an HPC. For my HPC, I signed into sftp(secure file transfer protocol) on the rivanna server then used the put command to put the file in my scratch directory.

------------------------------------------------------------------------------------
 Ex. code: put GCF_000006765.1_ASM676v1_genomic.fa /scratch/username/
 ------------------------------------------------------------------------------------
 
 
 
 
 When executing ls in the scratch directory, the file should now appear
 The index genome can now be constructed. Run the following script in order to generate the index:
 
 
 -------------------------------------------------------------------------------------
module load gcc
module load bowtie2
bowtie2-build filename.fa Desired_filename_genes

Ex. code
module load gcc
module load bowtie2
bowtie2-build bowtie2-build GCF_000006765.1_ASM676v1_genomic.fa Index_genome
-----------------------------------------------------------------------------------------




When running ls, 6 new files ending in .bt2 should have appeared bearing the name that you chose in the last step-
Index_genome.1.bt2                  
Index_genome.2.bt2                  
Index_genome.3.bt2                   
Index_genome.4.bt2                  
Index_genome.rev.1.bt2              
Index_genome.rev.2.bt2

This is the index Genome.
Some troubleshooting might be required here. For my project, the reference genome I chose created an invalid reference genome, so I had to download gene data from another webstie and format the file to work with my model. I used the same process as above, just with a different file. 





#Downloading Transcriptomic data sets
After finding the appropriate transcriptomic data sets, they will need to be downloaded onto the hpc. 

Every transcriptome on sra NCBI has a corresponding run key. These numbers strings typically start with "SRR." Make note of the run key(including the characters at the beginning), as this is what is used to download the transcriptome. 

Use the following code to create a script to downlaod any transcriptome to a desired hpc:

--------------------------------------------------------------------------------------------------------------------------------
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH -t 120:00:00#
SBATCH --mem=150000
#SBATCH --account=CSBLRivanna
#SBATCH --partition=standard

module load sratoolkit/2.10.5
fastq-dump --split-files ${1}

--------------------------------------------------------------------------------------------------------------------------------


I named my script sra.slurm. After writing the script use the following code to execut it for the desired data sets:

---------------------------------------------------------
for x in SRR#; do sbatch scriptname $x; done

ex. code: for x in SRR2080030 SRR7777123; do sbatch sra.slurm $x; done
---------------------------------------------------------


This will download and save the transcriptomes as their run key.fastq. Some download as single end reads and others download as paired end reads. Single ends will download in one file while paired ends will download in two. 

These are the transcriptomic data sets. 




#Downloading sickle
Next, on your local Ubuntu, download sickle. The tutorial link is below:
https://github.com/najoshi/sickle

After downloading, move the file to your hpc server, to the same directory as your transcriptomic data sets. Sickle is used soon. Note that you also have to download zlib for using sickle. Links to zlib here: http://www.zlib.net/






#Processing reads
Now, we want to construct a .csv or .tsv file that contains the gene ID, length, and # of hits. To do this , create the following script:
so first use nano "script name here" to create the script location-for uniformity, I have given all files the name of their purpose- name them whatever you like as long as you maintain consistency.

#First is the script for single end reads. 
-----------------------------------------------------------------------------------------------------------------------------------
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH -t 20:00:00
#SBATCH --mem=150000
#SBATCH --account=CSBLRivanna
#SBATCH --partition=standard

module load gcc/9.2.0 bowtie2/2.2.9
module load samtools/1.10

#Quality trim samples based on quality scores
#first is the location of your sickle file.
/location/of/sickle se -t sanger \
        -f input_SRR_file.fastq \
        -o intermediate_trim_file.fastq \
       
       
#Single Reads
bowtie2 -q --fr -x Index_genome \
        -U intermediate_trim_file.fastq \
        -S single.reads.sam
samtools view -h single.reads.sam -o single.reads.bam
samtools sort single.reads.bam -o single.reads.sort.bam
rm single.reads.bam single.reads.sam


#Sort again
samtools sort single.reads.sort.bam -o single.reads.sort2.bam

#readable format
samtools idxstats single.reads.sort2.bam > Output.tsv

-----------------------------------------------------------------------------------------------------------------------------------
HERE IS MY SAMPLE CODE FOR AN EXAMPLE SINGLE READS:

-----------------------------------------------------------------------------------------------------------------------------------
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH -t 20:00:00
#SBATCH --mem=150000
#SBATCH --account=CSBLRivanna
#SBATCH --partition=standard


module load gcc/9.2.0 bowtie2/2.2.9
module load samtools/1.10

#Quality trim samples based on quality scores
/scratch/rag6yy/sickle se -t sanger \
        -f SRR7777123_1.fastq \
        -o CFLung.trim.fastq \

#Single Reads
bowtie2 -q --fr -x Pseudomonas_genes \
        -U CFLung.trim.fastq \
        -S single.reads.sam
samtools view -h single.reads.sam -o single.reads.bam
samtools sort single.reads.bam -o single.reads.sort.bam
rm single.reads.bam single.reads.sam

#Sort again
samtools sort single.reads.sort.bam -o single.reads.sort2.bam

#readable format
samtools idxstats single.reads.sort2.bam > CF.lung.tsv
-----------------------------------------------------------------------------------------------------------------------------------




HERE IS MY GENERALIZED CODE FOR A LIST OF SINGLE READS: 

-----------------------------------------------------------------------------------------------------------------------------------
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH -t 20:00:00
#SBATCH --mem=150000
#SBATCH --account=CSBLRivanna
#SBATCH --partition=standard


module load gcc/9.2.0 bowtie2/2.2.9
module load samtools/1.10

#Quality trim samples based on quality scores
/scratch/rag6yy/sickle se -t sanger \
        -f ${1}_1.fastq \
        -o ${1}.trim.fastq \

#Single Reads
bowtie2 -q --fr -x Pseudomonas_genes \
        -U ${1}.trim.fastq \
        -S single.reads.sam
samtools view -h single.reads.sam -o single.reads.bam
samtools sort single.reads.bam -o single.reads.sort.bam
rm single.reads.bam single.reads.sam

#Sort again
samtools sort single.reads.sort.bam -o single.reads.sort2.bam


#readable format
samtools idxstats single.reads.sort2.bam > ${1}.tsv
rm single.reads.sort2.bam single.reads.sort.bam
-----------------------------------------------------------------------------------------------------------------------------------


##Then run 

-----------------------------------------------------------------------------------------------------------------------------------
for x in SRR7777123(or more, you could do all of your single read files here); do sbatch processing_single_reads $x; done
-----------------------------------------------------------------------------------------------------------------------------------



#Next is the script for paired end reads- it is not all that different. 

-----------------------------------------------------------------------------------------------------------------------------------
#!/bin/bash
#SBATCH--nodes=1
#SBATCH--ntasks-per-node=16
#SBATCH -t 20:00:00
#SBATCH --mem=150000
#SBATCH --account=CSBLRivanna
#SBATCH --partition=standard

module load gcc/9.2.0 bowtie2/2.2.9
module load samtools/1.10

#Quality trim samples based on quality scores
/location/of/sickle pe -t sanger -q 25 -l 21 \
        -f  Input_transcriptome_1.fastq \
        -r  Input_transcriptome_2.fastq \
        -o Input_transcriptome_1.trim.fastq \
        -p Input_transcriptome_2.trim.fastq \
        -s Orphan_reads.fastq
        
        
 #paired reads
bowtie2 -q --fr -x Index_genome
        -1 Input_transcriptome_1.trim.fastq \
        -2 Input_transcriptome_2.trim.fastq \
        -S pair.reads.sam
samtools view -bS pair.reads.sam > pair.reads.bam
samtools sort -o pair.reads.sort.bam pair.reads.bam
rm pair.reads.sam pair.reads.bam

#Orphaned reads
bowtie2 -q -x Index_genome
        -U Orphan_reads.fastq \
        -S orphanReads.sam
samtools view -bS orphanReads.sam > orphanReads.bam
samtools sort -o orphanReads.sort.bam orphanReads.bam
rm orphanReads.bam orphanReads.sam

#merge Alignments
samtools merge reads.merge.bam pair.reads.sort.bam orphanReads.sort.bam
samtools sort -o reads.merge.sort.bam reads.merge.bam
rm reads.merge.bam orphanReads.sort.bam pair.reads.sort.bam

#Readable format
samtools idxstats reads.merge.sort.bam > Output.tsv
rm reads.merge.sort.bam
-----------------------------------------------------------------------------------------------------------------------------------




HERE IS MY SAMPLE CODE FOR PAIRED ENDS: 

-----------------------------------------------------------------------------------------------------------------------------------
#!/bin/bash
#SBATCH--nodes=1
#SBATCH--ntasks-per-node=16
#SBATCH -t 20:00:00
#SBATCH --mem=150000
#SBATCH --account=CSBLRivanna
#SBATCH --partition=standard

module load gcc/9.2.0 bowtie2/2.2.9
module load samtools/1.10

#Quality trim samples based on quality scores
/scratch/rag6yy/sickle pe -t sanger -q 25 -l 21 \
        -f  SRR2080030_1.fastq \
        -r  SRR2080030_2.fastq \
        -o BurnWound1.trim.fastq \
        -p BurnWound2.trim.fastq \
        -s BurnOrphan.fastq
        
#paired reads
bowtie2 -q --fr -x Pseudomonas_genes \
        -1 BurnWound1.trim.fastq \
        -2 BurnWound2.trim.fastq \
        -S pair.reads.sam
samtools view -bS pair.reads.sam > pair.reads.bam
samtools sort -o pair.reads.sort.bam pair.reads.bam
rm pair.reads.sam pair.reads.bam

#Orphaned reads
bowtie2 -q -x Pseudomonas_genes \
        -U BurnOrphan.fastq \
        -S orphanReads.sam
samtools view -bS orphanReads.sam > orphanReads.bam
samtools sort -o orphanReads.sort.bam orphanReads.bam
rm orphanReads.bam orphanReads.sam

#merge Alignments
samtools merge reads.merge.bam pair.reads.sort.bam orphanReads.sort.bam
samtools sort -o reads.merge.sort.bam reads.merge.bam
rm reads.merge.bam orphanReads.sort.bam pair.reads.sort.bam

#Readable format
samtools idxstats reads.merge.sort.bam > burn_pseudomonas.tsv
rm reads.merge.sort.bam
-----------------------------------------------------------------------------------------------------------------------------------





#HERE IS MY CODE FOR A LIST OF PAIRED READS:

-----------------------------------------------------------------------------------------------------------------------------------
#!/bin/bash
#SBATCH--nodes=1
#SBATCH--ntasks-per-node=16
#SBATCH -t 20:00:00
#SBATCH --mem=150000
#SBATCH --account=CSBLRivanna
#SBATCH --partition=standard

module load gcc/9.2.0 bowtie2/2.2.9
module load samtools/1.10

#Quality trim samples based on quality scores
/scratch/rag6yy/sickle pe -t sanger -q 25 -l 21 \
        -f  ${1}_1.fastq \
        -r  ${1}_2.fastq \
        -o ${1}1.trim.fastq \
        -p ${1}2.trim.fastq \
        -s ${1}Orphan.fastq

#paired reads
bowtie2 -q --fr -x Pseudomonas_genes \
        -1 ${1}1.trim.fastq \
        -2 ${1}2.trim.fastq \
        -S pair.reads.sam
samtools view -bS pair.reads.sam > pair.reads.bam
samtools sort -o pair.reads.sort.bam pair.reads.bam
rm pair.reads.sam pair.reads.bam

#Orphaned reads
bowtie2 -q -x Pseudomonas_genes \
        -U ${1}Orphan.fastq \
        -S orphanReads.sam
samtools view -bS orphanReads.sam > orphanReads.bam
samtools sort -o orphanReads.sort.bam orphanReads.bam
rm orphanReads.bam orphanReads.sam

#merge Alignments
samtools merge reads.merge.bam pair.reads.sort.bam orphanReads.sort.bam
samtools sort -o reads.merge.sort.bam reads.merge.bam
rm reads.merge.bam orphanReads.sort.bam pair.reads.sort.bam

#Readable format
samtools idxstats reads.merge.sort.bam > ${1}.tsv
rm reads.merge.sort.bam
-----------------------------------------------------------------------------------------------------------------------------------






#Moving output
It's now time to move output files to jupyter notebook for analysis with RIPTiDe. Use the same sftp procedure as before but instead of put, use get. 
get /current/location/Output.tsv /desired/location/path
Here is my sample code:

-----------------------------------------------------------------------------------------------------------------------------------
 get /scratch/rag6yy/CF.lung.tsv /mnt/c/Users/rglad/Documents/REU Modeling
 -----------------------------------------------------------------------------------------------------------------------------------
 
 

#HERES HOW TO MOVE ALL .tsv's at once:

-----------------------------------------------------------------------------------------------------------------------------------
cd /location/of/files
get *.tsv /local/location
-----------------------------------------------------------------------------------------------------------------------------------

EXAMPLE CODE:

-----------------------------------------------------------------------------------------------------------------------------------
cd /scratch/rag6yy/
get *.tsv /mnt/c/Users/rglad/Documents
-----------------------------------------------------------------------------------------------------------------------------------


Once you get your output .tsv files into the same location as your GENRE, you are ready to create a contextualized model!



